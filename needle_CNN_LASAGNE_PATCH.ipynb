{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla K40c (CNMeM is enabled with initial size: 95.0% of memory, cuDNN 5004)\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import lasagne\n",
    "from copy import deepcopy\n",
    "\n",
    "import SimpleITK as sitk\n",
    "import random\n",
    "from skimage import exposure\n",
    "from skimage.morphology import binary_closing\n",
    "\n",
    "import sklearn.cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_patches(mri, label, half_patch_size=5):\n",
    "    \n",
    "    assert mri.shape == label.shape\n",
    "    \n",
    "    z_size, x_size, y_size = mri.shape\n",
    "    positive_patches = []\n",
    "    negative_patches = []\n",
    "\n",
    "    for z in xrange(half_patch_size, z_size-half_patch_size):\n",
    "        for x in xrange(half_patch_size, x_size-half_patch_size):\n",
    "            for y in xrange(half_patch_size, y_size-half_patch_size):\n",
    "                if label[z,x,y] == 1:\n",
    "                    positive_patches.append(mri[z-half_patch_size:z+half_patch_size,x-half_patch_size:x+half_patch_size,y-half_patch_size:y+half_patch_size])\n",
    "                elif label[z,x,y] == 0:\n",
    "                    negative_patches.append(mri[z-half_patch_size:z+half_patch_size,x-half_patch_size:x+half_patch_size,y-half_patch_size:y+half_patch_size])\n",
    "    \n",
    "    random.shuffle(negative_patches)\n",
    "    selected_negative_patches = deepcopy(negative_patches[:len(positive_patches)])\n",
    "    del(negative_patches)\n",
    "    \n",
    "    return (positive_patches, selected_negative_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def patchimg2differentview(patch):\n",
    "    \n",
    "    z_size, x_size, y_size = patch.shape\n",
    "    \n",
    "    X = np.zeros((z_size * 3, x_size, y_size), dtype=np.float32)\n",
    "    \n",
    "    counter = 0\n",
    "    for z in xrange(z_size):\n",
    "        X[counter,:,:] = patch[z,:,:]\n",
    "        counter += 1\n",
    "    for x in xrange(x_size):\n",
    "        X[counter,:,:] = patch[:,x,:]\n",
    "        counter += 1\n",
    "    for y in xrange(y_size):\n",
    "        X[counter,:,:] = patch[:,:,y]\n",
    "        counter += 1\n",
    "    \n",
    "    return X.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def patches2CNNformat(patches, label):\n",
    "    \n",
    "    X = np.zeros((len(patches), 30, 10, 10), dtype=np.float32)\n",
    "    y = np.zeros((len(patches)), dtype=np.int32) * -1\n",
    "    \n",
    "    for i, patch in enumerate(patches):\n",
    "        X[i,:,:,:] = patchimg2differentview(patch)\n",
    "        y[i] = label\n",
    "    \n",
    "    assert -1 not in y\n",
    "    \n",
    "    return X.astype(np.float32), y.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_resized_img(img_fn, data_type = sitk.sitkFloat32):\n",
    "    \n",
    "    img = sitk.ReadImage(img_fn)\n",
    "    size = img.GetSize()\n",
    "    ratio = [1.0/i for i in img.GetSpacing()]\n",
    "    new_size = [int(size[i]/ratio[i]) for i in range(3)]\n",
    "    \n",
    "    rimage = sitk.Image(new_size, data_type)\n",
    "    rimage.SetSpacing((1,1,1))\n",
    "    rimage.SetOrigin(img.GetOrigin())\n",
    "    tx = sitk.Transform()\n",
    "    \n",
    "    interp = sitk.sitkLinear\n",
    "    if data_type == sitk.sitkInt16:\n",
    "        interp = sitk.sitkNearestNeighbor\n",
    "    \n",
    "    new_image = sitk.Resample(img, rimage, tx, interp, data_type)\n",
    "    \n",
    "    #out_fn = img_fn.split(\"/\")[-1]\n",
    "    #sitk.WriteImage(new_image, \"resampled_IMG/\"+out_fn)\n",
    "    \n",
    "    return sitk.GetArrayFromImage(new_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def needles2tips(only_needles, image_array):\n",
    "    needles_masks_array = np.zeros_like(image_array).astype(float)  \n",
    "    for file_item in only_needles:\n",
    "        this_mask = file_item.astype(np.float)\n",
    "        if np.sum(this_mask) < (np.shape(this_mask)[0] * np.shape(this_mask)[1] * np.shape(this_mask)[2]):\n",
    "            this_mask = binary_closing(this_mask,selem=np.ones((3,3,3)))\n",
    "            found=False\n",
    "            row = 0#np.shape(this_mask)[0]-1\n",
    "            while found==False and row < np.shape(this_mask)[0]-1 :\n",
    "                #print(row)\n",
    "                this_row = this_mask[row,:,:]\n",
    "                if np.sum(this_row) > 0:\n",
    "                    #print(row)\n",
    "                    found = True\n",
    "                    temp = np.add(needles_masks_array[row:row+3,:,:],this_mask[row:row+3,:,:])\n",
    "                    temp[temp!=0] = 1\n",
    "                    needles_masks_array[row:row+3,:,:] = temp \n",
    "                row += 1\n",
    "    return needles_masks_array.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def data_for_CNN(general_folder):\n",
    "    folders_cases = os.listdir(general_folder)\n",
    "    \n",
    "    X = np.ones((1, 30, 10, 10), dtype=np.float32) * -1.0\n",
    "    y = np.ones((1), dtype=np.int32) * -1.0\n",
    "    \n",
    "    for folder_case in folders_cases:\n",
    "        print(\"Patient #%s\" % (folder_case))\n",
    "        full_case_path = general_folder + os.sep + folder_case\n",
    "        \n",
    "        volumetric_files = os.listdir(full_case_path)\n",
    "        \n",
    "        assert \"case.nrrd\" in volumetric_files\n",
    "        del(volumetric_files[volumetric_files.index(\"case.nrrd\")])\n",
    "        \n",
    "        mri = get_resized_img(full_case_path + os.sep + \"case.nrrd\")\n",
    "        \n",
    "        needles = []\n",
    "        for volumetric_file in volumetric_files:\n",
    "            label = get_resized_img(full_case_path + os.sep + volumetric_file, sitk.sitkInt16)\n",
    "            label[label!=0]=1\n",
    "            #if 1 in label: print(\"1!\")\n",
    "            needles.append(label)\n",
    "        \n",
    "        tips = needles2tips(needles, mri)\n",
    "        #tips[tips!=0]=1\n",
    "        #tips = tips.astype(np.int32)\n",
    "        positive_patches, negative_patches = extract_patches(mri, tips, half_patch_size=5)\n",
    "        if len(positive_patches) > 0:\n",
    "            print(\" %d positive patches and %d negative patches\" % (len(positive_patches), len(negative_patches)))\n",
    "            X_temp_pos, y_temp_pos = patches2CNNformat(positive_patches, 1)\n",
    "            X_temp_neg, y_temp_neg = patches2CNNformat(negative_patches, 0)\n",
    "            \n",
    "            X = np.concatenate((X, X_temp_pos), axis=0)\n",
    "            y = np.concatenate((y, y_temp_pos), axis=0)\n",
    "            X = np.concatenate((X, X_temp_neg), axis=0)\n",
    "            y = np.concatenate((y, y_temp_neg), axis=0)\n",
    "    \n",
    "    assert -1 in X[0,:,:,:] and y[0] == -1\n",
    "    X = X[1:,:,:,:].astype(np.float32)\n",
    "    y = y[1:].astype(np.int32)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#X, y = data_for_CNN(\"LabelMaps\")\n",
    "saved_data = np.load('patches_jun24.npz')\n",
    "X, y = saved_data['arr_0'], saved_data['arr_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5994, 30, 10, 10), (5994,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#np.savez('patches_jun24.npz', X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = sklearn.cross_validation.train_test_split(X, y, test_size=0.20)\n",
    "X_test, X_val, y_test, y_val = sklearn.cross_validation.train_test_split(X_test, y_test, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m, s = X_train.mean(), X_train.std()\n",
    "\n",
    "X_train -= m\n",
    "X_train /= s\n",
    "\n",
    "X_val -= m\n",
    "X_val /= s\n",
    "\n",
    "X_test -= m\n",
    "X_test /= s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4795, 30, 10, 10), (803, 30, 10, 10), (396, 30, 10, 10))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_cnn(single_entry_shape, input_var=None):\n",
    "    \n",
    "    network = lasagne.layers.InputLayer(shape=(None, single_entry_shape[0], single_entry_shape[1], \n",
    "                                               single_entry_shape[2]),\n",
    "                                        input_var=input_var)\n",
    "    \n",
    "    network = lasagne.layers.Conv2DLayer(\n",
    "            network, num_filters=60, filter_size=(3, 3), \n",
    "            nonlinearity=lasagne.nonlinearities.leaky_rectify,\n",
    "            W=lasagne.init.HeNormal())\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
    "    \n",
    "    network = lasagne.layers.Conv2DLayer(\n",
    "            network, num_filters=60, filter_size=(2, 2), \n",
    "            nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "    \n",
    "    network = lasagne.layers.DenseLayer(\n",
    "           lasagne.layers.dropout(network, p=.5),\n",
    "            num_units=60*2*2,\n",
    "           nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "    \n",
    "    network = lasagne.layers.DenseLayer(\n",
    "            lasagne.layers.dropout(network, p=.5),\n",
    "            num_units=2,\n",
    "            nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 550\n",
    "batchsize = 32\n",
    "single_entry_shape = X_train.shape[1:]\n",
    "\n",
    "# Prepare Theano variables for inputs and targets\n",
    "input_var = T.tensor4('inputs')\n",
    "target_var = T.ivector('targets')\n",
    "\n",
    "network = build_cnn(single_entry_shape, input_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a loss expression for training, i.e., a scalar objective we want\n",
    "# to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
    "prediction = lasagne.layers.get_output(network)\n",
    "loss = lasagne.objectives.categorical_crossentropy(prediction, target_var) # multiclass_hinge_loss\n",
    "loss = loss.mean()\n",
    "# We could add some weight decay as well here, see lasagne.regularization.\n",
    "\n",
    "# Create update expressions for training, i.e., how to modify the\n",
    "# parameters at each training step. Here, we'll use Stochastic Gradient\n",
    "# Descent (SGD) with Nesterov momentum, but Lasagne offers plenty more.\n",
    "params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "updates = lasagne.updates.nesterov_momentum(\n",
    "        loss, params, learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "# Create a loss expression for validation/testing. The crucial difference\n",
    "# here is that we do a deterministic forward pass through the network,\n",
    "# disabling dropout layers.\n",
    "test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "test_loss = lasagne.objectives.categorical_crossentropy(test_prediction, target_var)\n",
    "test_loss = test_loss.mean()\n",
    "# As a bonus, also create an expression for the classification accuracy:\n",
    "test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                    dtype=theano.config.floatX)\n",
    "\n",
    "# Compile a function performing a training step on a mini-batch (by giving\n",
    "# the updates dictionary) and returning the corresponding training loss:\n",
    "train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "\n",
    "# Compile a second function computing the validation loss and accuracy:\n",
    "val_fn = theano.function([input_var, target_var], [test_loss, test_acc])\n",
    "\n",
    "# Finally, launch the training loop.\n",
    "print(\"Starting training...\")\n",
    "# We iterate over epochs:\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    now = datetime.datetime.now()\n",
    "    print(\"Epoch %d started on %s\" % (epoch + 1, now.strftime(\"%Y-%m-%d %H:%M\")))\n",
    "    \n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X_train, y_train, batchsize, shuffle=True):\n",
    "        inputs, targets = batch\n",
    "        train_err += train_fn(inputs, targets)\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_err = 0\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(X_val, y_val, batchsize, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        err, acc = val_fn(inputs, targets)\n",
    "        val_err += err\n",
    "        val_acc += acc\n",
    "        val_batches += 1\n",
    "\n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))\n",
    "\n",
    "# After training, we compute and print the test error:\n",
    "test_err = 0\n",
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(X_test, y_test, batchsize, shuffle=False):\n",
    "    inputs, targets = batch\n",
    "    err, acc = val_fn(inputs, targets)\n",
    "    test_err += err\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "print(\"Final results:\")\n",
    "print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SAVE NETWORK\n",
    "#np.savez('model_needle_june24.npz', *lasagne.layers.get_all_param_values(network))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LOAD NETWORK\n",
    "#\"\"\"\n",
    "with np.load('model_needle_june24.npz') as f:\n",
    "    param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "lasagne.layers.set_all_param_values(network, param_values)\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "predict_fn = theano.function([input_var], T.argmax(test_prediction, axis=1))\n",
    "predict_confidence = theano.function([input_var], test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n"
     ]
    }
   ],
   "source": [
    "ind = 17\n",
    "a = X_test[ind,:,:,:].reshape((1,30,10,10))\n",
    "predict_label_patch = predict_fn(a)[0]\n",
    "\n",
    "print(predict_label_patch, y_test[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imshow(a[0,5,:,:], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 145, 130)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test_pat_sitk = sitk.ReadImage('LabelMaps/64/case.nrrd')\n",
    "test_pat = get_resized_img('LabelMaps/64/case.nrrd')\n",
    "test_pat = test_pat[150:,25:170,90:220]\n",
    "\n",
    "pat_sitk = sitk.GetImageFromArray(test_pat.astype(np.float32))\n",
    "sitk.WriteImage(pat_sitk, 'test_pat.nrrd')\n",
    "\n",
    "final_label = np.zeros_like(test_pat)\n",
    "test_pat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "half_patch_size = 5\n",
    "z_size, x_size, y_size = test_pat.shape\n",
    "\n",
    "for z in xrange(half_patch_size, z_size-half_patch_size):\n",
    "    print(z)\n",
    "    for x in xrange(half_patch_size, x_size-half_patch_size):\n",
    "        for y in xrange(half_patch_size, y_size-half_patch_size):\n",
    "            patient_patch_img = test_pat[z-half_patch_size:z+half_patch_size,x-half_patch_size:x+half_patch_size,y-half_patch_size:y+half_patch_size]\n",
    "            patient_patch = patchimg2differentview(patient_patch_img).reshape((1,30,10,10)).astype(np.float32)\n",
    "            patient_patch -= m\n",
    "            patient_patch /= s\n",
    "            predicted_label = int(predict_fn(patient_patch)[0])\n",
    "            \n",
    "            final_label[z,x,y] = predicted_label\n",
    "\n",
    "\n",
    "\n",
    "final_label_sitk = sitk.GetImageFromArray(final_label)\n",
    "sitk.WriteImage(final_label_sitk, 'test_label.nrrd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python2",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
